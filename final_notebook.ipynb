{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df20edc1",
   "metadata": {},
   "source": [
    "## Zillow Regression with Clustering Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ebf07",
   "metadata": {},
   "source": [
    "### Project Goals\n",
    "\n",
    "- The goal of this project is to find features or clusters of features to improve Zillow's log error for single family residences in three Southern California counties and to use these features to develop an improved machine learning model.\n",
    "\n",
    "- Our initial hypothesis is that the size of the home in square feet, the age of the home, and the location are the main features affecting log error.\n",
    "\n",
    "- Initial questions:\n",
    "    - What is the relationship between square feet and log error? Do area clusters have a large impact on the overall log error?\n",
    "    - Does the size of the home affect log error? Can that error be better determined by clustering by size?\n",
    "    - Does the location have an effect on log error? Where does the most log error occur?\n",
    "\n",
    "### Project Planning\n",
    "\n",
    "- Acquire the dataset from the Codeup database using SQL\n",
    "- Prepare the data with the intent to improve the log error from Zestimates; clean the data and encode categorical features if necessary; ensure that the data is tidy\n",
    "- Split the data into train, validate, and test datasets using a 60/20/20 split\n",
    "- Explore the data:\n",
    "    - Univariate, bivariate, and multivariate analyses; statistical tests for significance, find the three primary features or clusters affecting log error\n",
    "    - Create graphical representations of the analyses\n",
    "    - Answer questions about the data\n",
    "    - Document findings\n",
    "- Train and test at least three models:\n",
    "    - Establish a baseline\n",
    "    - Select key features and train multiple linear regression models\n",
    "    - Test the model on the validate set, adjust for overfitting if necessary\n",
    "- Select the best model for the project goals:\n",
    "    - Determine which model performs best on the validate set\n",
    "- Test and evaluate the model:\n",
    "    - Use the model on the test set and evaluate its performance (RMSE, R2, etc.)\n",
    "    - Visualize the data using an array of probabilities on the test set\n",
    "- Document key findings and takeaways, answer the questions\n",
    "    \n",
    "### Executive Summary\n",
    "\n",
    "- After running four models on my train and validate sets, we decided to use the polynomial linear regression model because it provided the lowest RMSE compared to baseline.\n",
    "\n",
    "- We selected the features for modeling based on statistical analysis (square feet of the home, ratio of bedrooms and bathrooms, lot size, age, number of bathrooms, area cluster, and size cluster). We selected a degree multiplier of 2. The RMSE of the selected model was .162 on train, .143 on validate, and .174 on test.\n",
    "\n",
    "- Takeaways: the selected features improved the overall log error, but not much more than baseline. The clusters did not significantly reduce the RMSE, but there was a very small improvement when using the absolute value for the log error. Overall, none of the models significantly outperformed Zillow's current model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc6aa3",
   "metadata": {},
   "source": [
    "### Acquire and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2547eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports for full data pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "from math import sqrt\n",
    "# imports for project-specific functions\n",
    "import env\n",
    "import wrangle\n",
    "import model\n",
    "import explore\n",
    "# sklearn imports for modeling, splitting, scaling\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression \n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full zillow database\n",
    "wrangle.full_zillow_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8237c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire database using function from wrangle.py and save to a variable\n",
    "df = wrangle.wrangle_zillow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee53c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify successful wrangling of data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ce093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify proper encoding of fips into counties\n",
    "wrangle.verify_counties(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6edeb5",
   "metadata": {},
   "source": [
    "### Acquisition and Preparation Takeaways\n",
    "\n",
    "- The dataset was acquired from the Codeup database using a SQL query.\n",
    "\n",
    "- Data was limited to homes with a transaction in 2017, homes with more than 0 and less than 8 bedrooms, more than 0 and less than 8 bathrooms, home size less than 10,000 square feet, homes with less than 20 acres, and homes with a tax rate less than 30. All observations with null values were removed. \n",
    "\n",
    "- FIPS was encoded and new features (age, age_bin, taxrate, acres, acres_bin, sqft_bin, structure_dollar_per_sqft, structure_dollar_per_sqft_bin, land_dollar_per_sqft, lot_dollar_sqft_bin) were created.\n",
    "\n",
    "- The cleaned dataset has 50699 observations and 29 columns. All columns are integers or floats.\n",
    "\n",
    "- The dataset has been split into train, validate, and test sets using a 60/20/20 split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bd0ac4",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f107e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validate, and test sets\n",
    "train, X_train, X_validate, X_test, y_train, y_validate, y_test = wrangle.split(df, target_var='logerror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0173e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin logerror\n",
    "train['logerror_bins'] = pd.cut(train.logerror, [-5, -.2, -.05, .05, .2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features and concatenate to train, validate, and test datasets; MinMax scaler was fit to train only\n",
    "X_train = explore.fit_scale_and_concat(X_train, X_train)\n",
    "X_validate = explore.fit_scale_and_concat(X_validate, X_train)\n",
    "X_test = explore.fit_scale_and_concat(X_test, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3224dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify successful concatenation\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e17788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of variables I will cluster on. \n",
    "cluster_vars = ['scaled_latitude', 'scaled_longitude', 'age_bin']\n",
    "# cluster column name\n",
    "cluster_name = 'area_cluster'\n",
    "# range for find_k\n",
    "k_range = range(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph find_k using the SSE\n",
    "explore.find_k(X_train, cluster_vars, k_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe69439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select best number for centroids according to elbow method\n",
    "k = 5\n",
    "# create the clusters using function from explore.py\n",
    "kmeans = explore.create_clusters(X_train, k, cluster_vars)\n",
    "# create dataframe for the clusters\n",
    "centroid_df = explore.get_centroids(kmeans, cluster_vars, cluster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c44048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the dataframe was created successfully\n",
    "centroid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the clusters to train, validate, and test for modeling\n",
    "X_train = explore.assign_clusters(X_train, kmeans, cluster_vars, cluster_name, centroid_df)\n",
    "X_validate = explore.assign_clusters(X_validate, kmeans, cluster_vars, cluster_name, centroid_df)\n",
    "X_test = explore.assign_clusters(X_test, kmeans, cluster_vars, cluster_name, centroid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the centroids and number of homes for each area cluster\n",
    "pd.DataFrame(X_train.groupby(['area_cluster', 'centroid_scaled_latitude', 'centroid_scaled_longitude', \n",
    "                           'centroid_age_bin'])['area_cluster'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features for size cluster\n",
    "cluster_vars = ['scaled_bathroomcnt', 'sqft_bin', 'acres_bin', 'bath_bed_ratio']\n",
    "cluster_name = 'size_cluster'\n",
    "k_range = range(2,10)\n",
    "# graph find_k using SSE to select the best k\n",
    "explore.find_k(X_train, cluster_vars, k_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select number of centroids using elbow method above\n",
    "k=5\n",
    "# fit kmeans \n",
    "kmeans = explore.create_clusters(X_train, k, cluster_vars)\n",
    "# get centroid values per variable per cluster\n",
    "centroid_df = explore.get_centroids(kmeans, cluster_vars, cluster_name)\n",
    "# get cluster assignments and append those with centroids for each X dataset (train, validate, test)\n",
    "X_train = explore.assign_clusters(X_train, kmeans, cluster_vars, cluster_name, centroid_df)\n",
    "X_validate = explore.assign_clusters(X_validate, kmeans, cluster_vars, cluster_name, centroid_df)\n",
    "X_test = explore.assign_clusters(X_test, kmeans, cluster_vars, cluster_name, centroid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the centroids and number of homes for each size cluster\n",
    "pd.DataFrame(X_train.groupby(['size_cluster', 'centroid_scaled_bathroomcnt', 'centroid_sqft_bin',\n",
    "                              'centroid_acres_bin', 'centroid_bath_bed_ratio'])['area_cluster'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9540c885",
   "metadata": {},
   "source": [
    "### Are the sizes of homes different based on area cluster? Are newer homes larger than older homes? Where are the newer homes located?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f59bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot visualizations of age of home and square footage for each area cluster\n",
    "explore.plot_age_sqft(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7cbb18",
   "metadata": {},
   "source": [
    "#### Statistical Test\n",
    "\n",
    "- H0: There is no relationship between the age of the home and the square feet of the home.\n",
    "- Ha: There is a relationship between the age of the home and the square feet of the home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d81e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct spearman test for age and square footage\n",
    "explore.spearman_test(X_train.age, X_train.scaled_calculatedfinishedsquarefeet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a6eeb0",
   "metadata": {},
   "source": [
    "- Spearman's correlation test shows that there is a negative relationship between the age of the home and square footage.\n",
    "- Los Angeles County has the largest number of homes over 60 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeaca31",
   "metadata": {},
   "source": [
    "### Does the age of the home affect log error? Do area clusters show any distinctions in log error and property age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c707504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph the relationship between age and log error for each area cluster\n",
    "explore.plot_age_error(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d0af8",
   "metadata": {},
   "source": [
    "#### Statistical Testing\n",
    "\n",
    "- H0: There is no relationship between the age of the home and the log error.\n",
    "- Ha: There is a relationship between the age of the home and the log error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f50c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct statistical test for relationship between age and logerror overall\n",
    "explore.spearman_test(X_train.age, y_train.logerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a17f6a",
   "metadata": {},
   "source": [
    "- Spearman's correlation test shows that there is a relationship between age and log error, but the correlation is very small (-0.05)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287c49d",
   "metadata": {},
   "source": [
    "### What is the relationship between square feet and log error? Do size clusters have a large impact on the overall log error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph the relationship between square footage and log error for each size cluster\n",
    "explore.plot_size_error(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b13230b",
   "metadata": {},
   "source": [
    "#### Statistical Testing\n",
    "\n",
    "- H0: There is no relationship between the number of square feet and log error.\n",
    "- Ha: There is a relationship between the number of square feet and log error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the relationship between square footage and log error\n",
    "explore.spearman_test(X_train.calculatedfinishedsquarefeet, y_train.logerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8fe8ca",
   "metadata": {},
   "source": [
    "- Spearman's correlation test confirms a relationship between square footage and log error, but once again the correlation coefficient is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca7952",
   "metadata": {},
   "source": [
    "### Does the location have an effect on log error? Where does the most log error occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph the relationship between square footage and log error for each county\n",
    "explore.plot_size_county_error(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct ANOVA statistical test for significance for square footage and log error between counties\n",
    "explore.anova_sqft_fips(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38267dbd",
   "metadata": {},
   "source": [
    "- Analysis of variance shows a significant difference in log error between counties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593ea13",
   "metadata": {},
   "source": [
    "### Exploration Takeaways\n",
    "\n",
    "- Analysis of variance shows a significant difference in home square footage between counties.\n",
    "- Initial exploration showed a moderate relationship between the size of the home and the home’s age, with Los Angeles County having the largest number of older homes. \n",
    "- The age and size of the home showed statistical significance to log error, but indicated a weak relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143291a",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25032ce1",
   "metadata": {},
   "source": [
    "- Select features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134114fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features for modeling and assign to a variable\n",
    "train_df = X_train[['scaled_calculatedfinishedsquarefeet', 'scaled_bathroomcnt', 'scaled_age',\n",
    "                        'size_cluster', 'area_cluster']]\n",
    "validate_df = X_validate[['scaled_calculatedfinishedsquarefeet', 'scaled_bathroomcnt', 'scaled_age', \n",
    "                        'size_cluster', 'area_cluster']]\n",
    "test_df = X_test[['scaled_calculatedfinishedsquarefeet', 'scaled_bathroomcnt', 'scaled_age',\n",
    "                        'size_cluster', 'area_cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish a baseline using logerror\n",
    "baseline = y_train.logerror.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8aa1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column for baseline in train set\n",
    "train_df['baseline'] = baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column for baseline in validate set\n",
    "validate_df['baseline'] = baseline\n",
    "RMSE_baseline = sqrt(mean_squared_error(y_train.logerror, train_df.baseline))\n",
    "RMSE_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and predict four models using selected features\n",
    "m1 = model.lasso_lars_model(train_df, validate_df, y_train, y_validate, 1.0)\n",
    "m2 = model.glm_model(train_df, validate_df, y_train, y_validate, 0, 1)\n",
    "m3 = model.poly_lm(train_df, validate_df, y_train, y_validate, 2)\n",
    "m4 = model.lrm(train_df, validate_df, y_train, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f9c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the performance for each model\n",
    "model.model_performance(m1,m2,m3,m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit and predict four models using the absolute value of log error\n",
    "m1 = model.lasso_lars_model(train_df, validate_df, np.absolute(y_train), np.absolute(y_validate), 1.0)\n",
    "m2 = model.glm_model(train_df, validate_df, np.absolute(y_train), np.absolute(y_validate), 2, 1)\n",
    "m3 = model.poly_lm(train_df, validate_df, np.absolute(y_train), np.absolute(y_validate), 2)\n",
    "m4 = model.lrm(train_df, validate_df, np.absolute(y_train), np.absolute(y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3454b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show each model's performance\n",
    "model.model_performance(m1,m2,m3,m4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c4bf2",
   "metadata": {},
   "source": [
    "## Test the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37243f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the features to polynomial regression\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "pf.fit(train_df)\n",
    "pf.transform(validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08528fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder column for test; this will be replaced with predicted values later\n",
    "test_df['yhat'] = baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the polynomial regression and graph the results; output RMSE and r2\n",
    "model.test_poly_lm(train_df, validate_df, test_df, y_train, y_validate, y_test, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254372f",
   "metadata": {},
   "source": [
    "#### Test Findings:\n",
    "\n",
    "- The linear regression with squared polynomial features performed above baseline with an RMSE of 0.174, which was  0.004 less error than the baseline prediction. \n",
    "- The model performed better on validate than on train, indicating that the model was not overfit to the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343c4bf",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations\n",
    "\n",
    "- All of the models performed very close to baseline, indicating that the new models with the selected features do not outperform Zillow’s current model. \n",
    "\n",
    "- Our recommendation would be to test Zillow’s current model using the absolute value of the log error if it does not do so already. \n",
    "\n",
    "- If we had more time, we would train the models after removing the homes that had the largest error to see how much the log error outliers affected the model. We would also see if clustering with log error as a feature can tell us more about why the model cannot accurately predict certain home values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddcec6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
